[E ProcessGroupNCCL.cpp:719] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11117, OpType=ALLGATHER, Timeout(ms)=1800000) ran for 1802008 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:719] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11117, OpType=ALLGATHER, Timeout(ms)=1800000) ran for 1802139 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:406] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down.
[E ProcessGroupNCCL.cpp:719] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11117, OpType=ALLGATHER, Timeout(ms)=1800000) ran for 1802375 milliseconds before timing out.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11117, OpType=ALLGATHER, Timeout(ms)=1800000) ran for 1802139 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:406] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11117, OpType=ALLGATHER, Timeout(ms)=1800000) ran for 1802008 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:406] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=11117, OpType=ALLGATHER, Timeout(ms)=1800000) ran for 1802375 milliseconds before timing out.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 64905 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 0 (pid: 64903) of binary: /ext3/miniconda3/bin/python3.9
Traceback (most recent call last):
  File "/ext3/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
training.main FAILED
------------------------------------------------------
Failures:
[1]:
  time      : 2022-08-02_19:00:44
  host      : gm005.hpc.nyu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 64904)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 64904
[2]:
  time      : 2022-08-02_19:00:44
  host      : gm005.hpc.nyu.edu
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 64906)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 64906
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-08-02_19:00:44
  host      : gm005.hpc.nyu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 64903)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 64903
======================================================
Singularity> /ext3/miniconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '

***

[W ProcessGroupNCCL.cpp:847] [Rank 1] Found key in store: NCCLABORTEDCOMM:20976fa0719000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, from rank: 6. This means that rank has aborted its NCCL communicators previously and is not in a healthy state.. Aborting appropriate communicators
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
        return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,

  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
    return _run_code(code, main_globals, None,
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
        exec(code, run_globals)exec(code, run_globals)

  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
        return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,    

exec(code, run_globals)  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code

  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
        exec(code, run_globals)exec(code, run_globals)

  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    return _run_code(code, main_globals, None,
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
    return _run_code(code, main_globals, None,
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
    exec(code, run_globals)
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 183, in train_one_epoch
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    main()    
train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
  File "/scratch/bf996/open_clip/src/training/train.py", line 183, in train_one_epoch
    main()
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
  File "/scratch/bf996/open_clip/src/training/train.py", line 183, in train_one_epoch
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 183, in train_one_epoch
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 183, in train_one_epoch
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 183, in train_one_epoch
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 183, in train_one_epoch
    #"TEXTS" is actually another image file, in the case of SIMCLR                    
  File "/scratch/bf996/open_clip/src/training/train.py", line 52, in train_integer_labels
    #"TEXTS" is actually another image file, in the case of SIMCLR                    
  File "/scratch/bf996/open_clip/src/training/train.py", line 52, in train_integer_labels
            #"TEXTS" is actually another image file, in the case of SIMCLR                    #"TEXTS" is actually another image file, in the case of SIMCLR                    #"TEXTS" is actually another image file, in the case of SIMCLR                    

    
  File "/scratch/bf996/open_clip/src/training/train.py", line 52, in train_integer_labels
  File "/scratch/bf996/open_clip/src/training/train.py", line 52, in train_integer_labels
#"TEXTS" is actually another image file, in the case of SIMCLR                      File "/scratch/bf996/open_clip/src/training/train.py", line 52, in train_integer_labels

    #"TEXTS" is actually another image file, in the case of SIMCLR                      File "/scratch/bf996/open_clip/src/training/train.py", line 52, in train_integer_labels

  File "/scratch/bf996/open_clip/src/training/train.py", line 52, in train_integer_labels
    return loss(logits, labels)
      File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return loss(logits, labels)    
return loss(logits, labels)
      File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
return loss(logits, labels)  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
        return loss(logits, labels)return loss(logits, labels)

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return loss(logits, labels)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
              File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)


  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
    return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
      File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
        logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)

  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
      File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
            return _AllGather.apply(group, tensor)return _AllGather.apply(group, tensor)return _AllGather.apply(group, tensor)


  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)    
all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
            dist.all_gather(out_tensor_list, tensor, group=group)    dist.all_gather(out_tensor_list, tensor, group=group)return _AllGather.apply(group, tensor)

dist.all_gather(out_tensor_list, tensor, group=group)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
          File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
return _AllGather.apply(group, tensor)return _AllGather.apply(group, tensor)

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
    return _AllGather.apply(group, tensor)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
    dist.all_gather(out_tensor_list, tensor, group=group)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
        dist.all_gather(out_tensor_list, tensor, group=group)dist.all_gather(out_tensor_list, tensor, group=group)

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
    dist.all_gather(out_tensor_list, tensor, group=group)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
        work.wait()                    
work.wait()work.wait()work.wait()work.wait()work.wait()work.wait()




RuntimeError
: [Rank 0] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=1800000) ran for 1800007 milliseconds before timing out.RuntimeError
: RuntimeErrorRuntimeErrorRuntimeErrorRuntimeErrorRuntimeError[Rank 5] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=1800000) ran for 1800009 milliseconds before timing out.: : [Rank 6] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=1800000) ran for 1800004 milliseconds before timing out.: : : 
[Rank 4] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=1800000) ran for 1800004 milliseconds before timing out.[Rank 3] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=1800000) ran for 1800005 milliseconds before timing out.
[Rank 1] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=1800000) ran for 1800004 milliseconds before timing out.[Rank 2] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=1800000) ran for 1800002 milliseconds before timing out.



Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 222, in train_one_epoch
    optimizer.zero_grad()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 288, in backward
    gxs = _AlltoAll.apply(ctx.group, tensor_list, *grad_outputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 309, in forward
    dist.all_to_all(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2734, in all_to_all
    work.wait()
RuntimeError: [Rank 7] Caught collective operation timeout: WorkNCCL(SeqNum=5541, OpType=ALLTOALL, TensorShape=[115, 1000], Timeout(ms)=1800000) ran for 1800005 milliseconds before timing out.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3440960) of binary: /ext3/miniconda3/bin/python3.9
Traceback (most recent call last):
  File "/ext3/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
training.main FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-08-03_06:39:39
  host      : gm016.hpc.nyu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3440961)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2022-08-03_06:39:39
  host      : gm016.hpc.nyu.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3440962)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2022-08-03_06:39:39
  host      : gm016.hpc.nyu.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3440963)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2022-08-03_06:39:39
  host      : gm016.hpc.nyu.edu
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 3440964)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2022-08-03_06:39:39
  host      : gm016.hpc.nyu.edu
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 3440965)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2022-08-03_06:39:39
  host      : gm016.hpc.nyu.edu
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 3440966)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2022-08-03_06:39:39
  host      : gm016.hpc.nyu.edu
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 3440967)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-08-03_06:39:39
  host      : gm016.hpc.nyu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3440960)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

***

raceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
                        return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,





  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
                exec(code, run_globals)exec(code, run_globals)        exec(code, run_globals)exec(code, run_globals)


exec(code, run_globals)exec(code, run_globals)

  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>

  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
    return _run_code(code, main_globals, None,
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
        main()
main()  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main

  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
            main()main()train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)


  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
  File "/scratch/bf996/open_clip/src/training/train.py", line 182, in train_one_epoch
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 182, in train_one_epoch
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 182, in train_one_epoch
        total_loss = train_integer_labels(unwrap_model(model).visual, images, texts, device, loss)train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)

      File "/scratch/bf996/open_clip/src/training/train.py", line 53, in train_integer_labels
total_loss = train_integer_labels(unwrap_model(model).visual, images, texts, device, loss)  File "/scratch/bf996/open_clip/src/training/train.py", line 182, in train_one_epoch

  File "/scratch/bf996/open_clip/src/training/train.py", line 53, in train_integer_labels
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 182, in train_one_epoch
        total_loss = train_integer_labels(unwrap_model(model).visual, images, texts, device, loss)train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)

  File "/scratch/bf996/open_clip/src/training/train.py", line 53, in train_integer_labels
  File "/scratch/bf996/open_clip/src/training/train.py", line 182, in train_one_epoch
    return loss(logits, labels)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return loss(logits, labels)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
        return loss(logits, labels)total_loss = train_integer_labels(unwrap_model(model).visual, images, texts, device, loss)
    
total_loss = train_integer_labels(unwrap_model(model).visual, images, texts, device, loss)  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/scratch/bf996/open_clip/src/training/train.py", line 53, in train_integer_labels

  File "/scratch/bf996/open_clip/src/training/train.py", line 53, in train_integer_labels
    total_loss = train_integer_labels(unwrap_model(model).visual, images, texts, device, loss)
  File "/scratch/bf996/open_clip/src/training/train.py", line 53, in train_integer_labels
        return loss(logits, labels)return loss(logits, labels)

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
        return loss(logits, labels)train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
  File "/scratch/bf996/open_clip/src/training/train.py", line 182, in train_one_epoch
    total_loss = train_integer_labels(unwrap_model(model).visual, images, texts, device, loss)
  File "/scratch/bf996/open_clip/src/training/train.py", line 53, in train_integer_labels
    return loss(logits, labels)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

      File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
return forward_call(*input, **kwargs)  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward

  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
    return forward_call(*input, **kwargs)    
return forward_call(*input, **kwargs)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 206, in forward
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
    logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)    
logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features

  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
        all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)logits, labels = gather_features(logits, labels, gather_with_grad=self.args.gather_with_grad, local_loss=self.args.local_loss)

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
  File "/scratch/bf996/open_clip/src/open_clip/loss.py", line 51, in gather_features
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)    
all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
    all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 113, in all_gather
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
                return _AllGather.apply(group, tensor)    return _AllGather.apply(group, tensor)return _AllGather.apply(group, tensor)return _AllGather.apply(group, tensor)
return _AllGather.apply(group, tensor)



  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
    return _run_code(code, main_globals, None,
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/scratch/bf996/open_clip/src/training/main.py", line 373, in <module>
                dist.all_gather(out_tensor_list, tensor, group=group)dist.all_gather(out_tensor_list, tensor, group=group)dist.all_gather(out_tensor_list, tensor, group=group)dist.all_gather(out_tensor_list, tensor, group=group)    



dist.all_gather(out_tensor_list, tensor, group=group)  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
        return _AllGather.apply(group, tensor)return _AllGather.apply(group, tensor)

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 282, in forward
        dist.all_gather(out_tensor_list, tensor, group=group)dist.all_gather(out_tensor_list, tensor, group=group)

  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2067, in all_gather
    work.wait()
        work.wait()work.wait()

RuntimeError    : work.wait()[Rank 0] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=600000) ran for 600001 milliseconds before timing out.
RuntimeErrorRuntimeError
: : [Rank 2] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=600000) ran for 600002 milliseconds before timing out.[Rank 1] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=600000) ran for 600003 milliseconds before timing out.
RuntimeError
: [Rank 6] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=600000) ran for 600006 milliseconds before timing out.
        work.wait()work.wait()

RuntimeErrorRuntimeError: : [Rank 5] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=600000) ran for 600009 milliseconds before timing out.[Rank 4] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=600000) ran for 600006 milliseconds before timing out.

    work.wait()
RuntimeError: [Rank 3] Caught collective operation timeout: WorkNCCL(SeqNum=5540, OpType=ALLGATHER, TensorShape=[8, 128, 25], Timeout(ms)=600000) ran for 600008 milliseconds before timing out.
    main()
  File "/scratch/bf996/open_clip/src/training/main.py", line 321, in main
    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer, args.sim_clr)
  File "/scratch/bf996/open_clip/src/training/train.py", line 220, in train_one_epoch
    scaler.scale(total_loss).backward()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 288, in backward
    gxs = _AlltoAll.apply(ctx.group, tensor_list, *grad_outputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/nn/functional.py", line 309, in forward
    dist.all_to_all(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 2734, in all_to_all
    work.wait()
RuntimeError: [Rank 7] Caught collective operation timeout: WorkNCCL(SeqNum=5541, OpType=ALLTOALL, TensorShape=[115, 1000], Timeout(ms)=600000) ran for 600003 milliseconds before timing out.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3699841) of binary: /ext3/miniconda3/bin/python3.9
Traceback (most recent call last):
  File "/ext3/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
training.main FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-08-03_10:00:37
  host      : gm016.hpc.nyu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3699842)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2022-08-03_10:00:37
  host      : gm016.hpc.nyu.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3699843)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2022-08-03_10:00:37
  host      : gm016.hpc.nyu.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3699844)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2022-08-03_10:00:37
  host      : gm016.hpc.nyu.edu
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 3699845)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2022-08-03_10:00:37
  host      : gm016.hpc.nyu.edu
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 3699853)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2022-08-03_10:00:37
  host      : gm016.hpc.nyu.edu
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 3699868)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2022-08-03_10:00:37
  host      : gm016.hpc.nyu.edu
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 3699869)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-08-03_10:00:37
  host      : gm016.hpc.nyu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3699841)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

***

    'export PYTHONPATH="$PYTHONPATH:$PWD/src"; python src/training/main.py --report-to tensorboard --train-data "/vast/work/public/ml-datasets/laion400m/{10000..11500}.tar" --train-num-samples 15000000 --dataset-type webdataset --integer-labels --multiclass --ds-filter="imagenet_classnames" --imagenet-a "/imagenet-a" --imagenet-r "/imagenet-r" --imagenet-val "/imagenet/val/" --imagenet-v2 "/scratch/bf996/datasets" --imagenet-s "/imagenet-sketch" --zeroshot-frequency=4 --save-frequency 1 --warmup 2000 --batch-size=128 --epochs=32 --workers=4 --model=RN50-in1k --resume "/scratch/bf996/open_clip/logs/laion15m-in1k-integerlabels-multiclass-ep8-14/checkpoints/epoch_14.pt" --clamp=1e5 --gather-with-grad --local-loss'
        'export PYTHONPATH="$PYTHONPATH:$PWD/src"; python src/training/main.py --report-to wandb --train-data "/vast/work/public/ml-datasets/laion400m/{30000..40000}.tar" --train-num-samples 100000000 --dataset-type webdataset --integer-labels --multiclass --strict=True --ds-filter="imagenet_classnames" --imagenet-a "/imagenet-a" --imagenet-r "/imagenet-r" --imagenet-val "/imagenet/val/" --imagenet-v2 "/scratch/bf996/datasets" --imagenet-s "/imagenet-sketch" --zeroshot-frequency=4 --save-frequency 1 --warmup 2000 --batch-size=128 --clamp=1e5 --epochs=32 --workers=4 --model=RN50-in1k --clamp=1e5 --gather-with-grad --local-loss'